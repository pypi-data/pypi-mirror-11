Metadata-Version: 1.1
Name: webb
Version: 0.9.1.2
Summary: An all-in-one Web Crawler, Web Parser and Web Scrapping library!
Home-page: https://github.com/hardikvasa/webb
Author: Hardik Vasa
Author-email: hnvasa@gmail.com
License: Apache License, Version 2.0
Description: ## Webb - A Complete Web Scrapper and Crawler Library
        An all-in-one Python library to scrap, parse and crawl web pages
        
        ### Gist
        This is a light-weight, dynamic and highly-flexible Python library. It can be used to **crawl, download, index, parse, scrap and analyze web pages** in a systematic manner or any of the individual function. It is also used to **clean web pages, store web data** and **import/export relevant components** from the web. Some of the other features also include **downloading google images and spidering wikipedia articles**.
        
        ### Usage and Instructions
        For usage and instructions please visit the [Official Documentation](docs/Documentation.md)
        
        For issues and discussion visit the [Issue Tracker](https://github.com/hardikvasa/webb/issues)
        
        For sample codes and examples, please visit [Examples Codes](examples)
        
        
        ### Compatability
        This library is compatible with both Python 2 (2.x) as well as Python 3 (3.x) versions. It is a download-import-and-run program with no or little changes as required by users.
        
        ### Dependencies
        There are **no dependencies** to this project. It functions entirely of the standard in-build library support. It does not need any external support or installations. Just download and run!!!
        
        ### Status
        This is a stand-alone python script which is ready-to-run, but still under development. Many more features will be added to it shortly.
        
        ### Disclaimer
        The crawler function lets you download  and crawl tons of web pages. Please do not download and crawl any pages of a domain without reading about the robot.txt file of that domain. 
        
        It is inappropriate to violate the robot.txt file. This may even lead to the domain completely blocking your crawler and thus blacklisting it. It is also not appropriate to crawl pages at high rate as it may put a lot of pressure on the server.
        
Keywords: scraper crawler spider webb
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: Programming Language :: Python :: 2
Classifier: Programming Language :: Python :: 2.6
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.3
Classifier: Programming Language :: Python :: 3.4
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Environment :: Web Environment
Classifier: Topic :: Internet :: WWW/HTTP
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Classifier: Topic :: Software Development :: Libraries :: Python Modules
