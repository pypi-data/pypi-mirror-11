<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">




<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta property="description" content="Efficient topic modelling of text semantics in Python." />
    <meta property="og:title" content="gensim: topic modelling for humans" />
    <meta property="og:description" content="Efficient topic modelling in Python" />

    
      <title>gensim: models.word2vec – Deep learning with word2vec</title>

    
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/jquery.qtip.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/anythingslider.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-24066335-1']);
      _gaq.push(['_trackPageview']);

      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>

  </head>

  <body>
    <div id="topwrap">
      
      <div id="top1">
        <div id="left1">
          <h1 class="h1gensim">
            <img src="../_static/images/logo-gensim_compact.png" alt="gensim logo" title="Gensim - topic modelling for humans" />
          </h1>
        </div>

        <div id="middleright">
          <div id="middle1">
            <div id="gensim"><a href="../index.html"><img src="../_static/images/gensim_compact.png" alt="gensim" title="Gensim home" /></a></div>
            <div id="tagline"><img src="../_static/images/tagline_compact.png" alt="gensim tagline" /></div>
          </div>
          <div id="right1">
            <div class="consulting-banner">
              <h3><a href="http://radimrehurek.com/">Get Expert Help</a></h3>
              <p>• machine learning, NLP, data mining</p>
              <p>• custom SW design, development, optimizations</p>
              <p>• tech trainings &amp; IT consulting</p>
            </div>
          </div>
        </div>
      </div>
     

      
      <div id="menu">
        <div id="indentation1">
          <ul class="menubuttons">
            <li class="menubutton"><a href="../index.html">Home</a></li>
            <li class="menubutton"><a href="../tutorial.html">Tutorials</a></li>
            <li class="menubutton"><a href="../install.html">Install</a></li>
            <li class="menubutton"><a href="../support.html">Support</a></li>
            <li class="menubutton"><a href="../apiref.html">API</a></li>
            <li class="menubutton"><a href="../about.html">About</a></li>
          </ul>
        </div>
      </div>
      

      <div class="clearer"></div>
    </div>

    
  <script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
    URL_ROOT: '../',
    VERSION: '0.12.2',
    COLLAPSE_INDEX: false,
    FILE_SUFFIX: '.html',
    HAS_SOURCE: true
  };
  </script>
    <script type="text/javascript" src="../_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.qtip.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-migrate-1.1.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.anythingslider.min.js"></script>

    
    <div class="document">
      
        <div id="thinbanner">
          <div id="bodythinbanner">
            <span class="h2gensim">models.word2vec – Deep learning with word2vec</span>
          </div>
        </div>
        <div class="obsah">
          <div class="obsahwrapper">
            
  <div class="section" id="module-gensim.models.word2vec">
<span id="models-word2vec-deep-learning-with-word2vec"></span><h1><code class="xref py py-mod docutils literal"><span class="pre">models.word2vec</span></code> &#8211; Deep learning with word2vec<a class="headerlink" href="#module-gensim.models.word2vec" title="Permalink to this headline">¶</a></h1>
<p>Deep learning via word2vec&#8217;s &#8220;skip-gram and CBOW models&#8221;, using either
hierarchical softmax or negative sampling <a class="footnote-reference" href="#id4" id="id1">[1]</a> <a class="footnote-reference" href="#id5" id="id2">[2]</a>.</p>
<p>The training algorithms were originally ported from the C package <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a>
and extended with additional functionality.</p>
<p>For a blog tutorial on gensim word2vec, with an interactive web app trained on GoogleNews, visit <a class="reference external" href="http://radimrehurek.com/2014/02/word2vec-tutorial/">http://radimrehurek.com/2014/02/word2vec-tutorial/</a></p>
<p><strong>Make sure you have a C compiler before installing gensim, to use optimized (compiled) word2vec training</strong>
(70x speedup compared to plain NumPy implementation <a class="footnote-reference" href="#id6" id="id3">[3]</a>).</p>
<p>Initialize a model with e.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Persist a model to disk with:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>  <span class="c"># you can continue training with the loaded model!</span>
</pre></div>
</div>
<p>The model can also be instantiated from an existing file on disk in the word2vec C format:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s">&#39;/tmp/vectors.txt&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c"># C text format</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s">&#39;/tmp/vectors.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># C binary format</span>
</pre></div>
</div>
<p>You can perform various syntactic/semantic NLP word tasks with the model. Some of them
are already built-in:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;woman&#39;</span><span class="p">,</span> <span class="s">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;man&#39;</span><span class="p">])</span>
<span class="go">[(&#39;queen&#39;, 0.50882536), ...]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s">&quot;breakfast cereal dinner lunch&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">&#39;cereal&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">&#39;woman&#39;</span><span class="p">,</span> <span class="s">&#39;man&#39;</span><span class="p">)</span>
<span class="go">0.73723527</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">[</span><span class="s">&#39;computer&#39;</span><span class="p">]</span>  <span class="c"># raw numpy vector of a word</span>
<span class="go">array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)</span>
</pre></div>
</div>
<p>and so on.</p>
<p>If you&#8217;re finished training a model (=no more updates, only querying), you can do</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">init_sims</span><span class="p">(</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>to trim unneeded model memory = use (much) less RAM.</p>
<p>Note that there is a <a class="reference internal" href="phrases.html#module-gensim.models.phrases" title="gensim.models.phrases: Phrase (collocation) detection"><code class="xref py py-mod docutils literal"><span class="pre">gensim.models.phrases</span></code></a> module which lets you automatically
detect phrases longer than one word. Using phrases, you can learn a word2vec model
where &#8220;words&#8221; are actually multiword expressions, such as <cite>new_york_times</cite> or <cite>financial_crisis</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">bigram_transformer</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Phrases</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">bigram_transformed</span><span class="p">[</span><span class="n">sentences</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality.
In Proceedings of NIPS, 2013.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Optimizing word2vec in gensim, <a class="reference external" href="http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/">http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/</a></td></tr>
</tbody>
</table>
<dl class="class">
<dt id="gensim.models.word2vec.BrownCorpus">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">BrownCorpus</code><span class="sig-paren">(</span><em>dirname</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.BrownCorpus" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Iterate over sentences from the Brown corpus (part of NLTK data).</p>
</dd></dl>

<dl class="class">
<dt id="gensim.models.word2vec.FakeJobQueue">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">FakeJobQueue</code><span class="sig-paren">(</span><em>init_fn</em>, <em>job_fn</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.FakeJobQueue" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Pretends to be a Queue; does equivalent of work_loop in calling thread.</p>
<dl class="method">
<dt id="gensim.models.word2vec.FakeJobQueue.put">
<code class="descname">put</code><span class="sig-paren">(</span><em>job</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.FakeJobQueue.put" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="gensim.models.word2vec.LineSentence">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">LineSentence</code><span class="sig-paren">(</span><em>source</em>, <em>max_sentence_length=10000</em>, <em>limit=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.LineSentence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Simple format: one sentence = one line; words already preprocessed and separated by whitespace.</p>
<p><cite>source</cite> can be either a string or a file object. Clip the file to the first
<cite>limit</cite> lines (or no clipped if limit is None, the default).</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sentences</span> <span class="o">=</span> <span class="n">LineSentence</span><span class="p">(</span><span class="s">&#39;myfile.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Or for compressed files:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sentences</span> <span class="o">=</span> <span class="n">LineSentence</span><span class="p">(</span><span class="s">&#39;compressed_text.txt.bz2&#39;</span><span class="p">)</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">LineSentence</span><span class="p">(</span><span class="s">&#39;compressed_text.txt.gz&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="gensim.models.word2vec.Text8Corpus">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">Text8Corpus</code><span class="sig-paren">(</span><em>fname</em>, <em>max_sentence_length=1000</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Text8Corpus" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Iterate over sentences from the &#8220;text8&#8221; corpus, unzipped from <a class="reference external" href="http://mattmahoney.net/dc/text8.zip">http://mattmahoney.net/dc/text8.zip</a> .</p>
</dd></dl>

<dl class="class">
<dt id="gensim.models.word2vec.Vocab">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">Vocab</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info,
and for constructing binary trees (incl. both word leaves and inner nodes).</p>
</dd></dl>

<dl class="class">
<dt id="gensim.models.word2vec.Word2Vec">
<em class="property">class </em><code class="descclassname">gensim.models.word2vec.</code><code class="descname">Word2Vec</code><span class="sig-paren">(</span><em>sentences=None</em>, <em>size=100</em>, <em>alpha=0.025</em>, <em>window=5</em>, <em>min_count=5</em>, <em>max_vocab_size=None</em>, <em>sample=0</em>, <em>seed=1</em>, <em>workers=1</em>, <em>min_alpha=0.0001</em>, <em>sg=1</em>, <em>hs=1</em>, <em>negative=0</em>, <em>cbow_mean=0</em>, <em>hashfxn=&lt;built-in function hash&gt;</em>, <em>iter=1</em>, <em>null_word=0</em>, <em>trim_rule=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../utils.html#gensim.utils.SaveLoad" title="gensim.utils.SaveLoad"><code class="xref py py-class docutils literal"><span class="pre">gensim.utils.SaveLoad</span></code></a></p>
<p>Class for training, using and evaluating neural networks described in <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a></p>
<p>The model can be stored/loaded via its <cite>save()</cite> and <cite>load()</cite> methods, or stored/loaded in a format
compatible with the original word2vec implementation via <cite>save_word2vec_format()</cite> and <cite>load_word2vec_format()</cite>.</p>
<p>Initialize the model from an iterable of <cite>sentences</cite>. Each sentence is a
list of words (unicode strings) that will be used for training.</p>
<p>The <cite>sentences</cite> iterable can be simply a list, but for larger corpora,
consider an iterable that streams the sentences directly from disk/network.
See <a class="reference internal" href="#gensim.models.word2vec.BrownCorpus" title="gensim.models.word2vec.BrownCorpus"><code class="xref py py-class docutils literal"><span class="pre">BrownCorpus</span></code></a>, <a class="reference internal" href="#gensim.models.word2vec.Text8Corpus" title="gensim.models.word2vec.Text8Corpus"><code class="xref py py-class docutils literal"><span class="pre">Text8Corpus</span></code></a> or <a class="reference internal" href="#gensim.models.word2vec.LineSentence" title="gensim.models.word2vec.LineSentence"><code class="xref py py-class docutils literal"><span class="pre">LineSentence</span></code></a> in
this module for such examples.</p>
<p>If you don&#8217;t supply <cite>sentences</cite>, the model is left uninitialized &#8211; use if
you plan to initialize it in some other way.</p>
<p><cite>sg</cite> defines the training algorithm. By default (<cite>sg=1</cite>), skip-gram is used.
Otherwise, <cite>cbow</cite> is employed.</p>
<p><cite>size</cite> is the dimensionality of the feature vectors.</p>
<p><cite>window</cite> is the maximum distance between the current and predicted word within a sentence.</p>
<p><cite>alpha</cite> is the initial learning rate (will linearly drop to zero as training progresses).</p>
<p><cite>seed</cite> = for the random number generator. Initial vectors for each
word are seeded with a hash of the concatenation of word + str(seed).</p>
<p><cite>min_count</cite> = ignore all words with total frequency lower than this.</p>
<p><cite>max_vocab_size</cite> = limit RAM during vocabulary building; if there are more unique
words than this, then prune the infrequent ones. Every 10 million word types
need about 1GB of RAM. Set to <cite>None</cite> for no limit (default).</p>
<dl class="docutils">
<dt><cite>sample</cite> = threshold for configuring which higher-frequency words are randomly downsampled;</dt>
<dd>default is 0 (off), useful value is 1e-5.</dd>
</dl>
<p><cite>workers</cite> = use this many worker threads to train the model (=faster training with multicore machines).</p>
<p><cite>hs</cite> = if 1 (default), hierarchical sampling will be used for model training (else set to 0).</p>
<p><cite>negative</cite> = if &gt; 0, negative sampling will be used, the int for negative
specifies how many &#8220;noise words&#8221; should be drawn (usually between 5-20).</p>
<p><cite>cbow_mean</cite> = if 0 (default), use the sum of the context word vectors. If 1, use the mean.
Only applies when cbow is used.</p>
<p><cite>hashfxn</cite> = hash function to use to randomly initialize weights, for increased
training reproducibility. Default is Python&#8217;s rudimentary built in hash function.</p>
<p><cite>iter</cite> = number of iterations (epochs) over the corpus.</p>
<dl class="docutils">
<dt><cite>trim_rule</cite> = vocabulary trimming rule, specifies whether certain words should remain</dt>
<dd><p class="first">in the vocabulary, be trimmed away, or handled using the default (discard if word count &lt; min_count).
Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and
returns either util.RULE_DISCARD, util.RULE_KEEP or util.RULE_DEFAULT.
Note: The rule, if given, is only used prune vocabulary during build_vocab() and is not stored as part</p>
<blockquote class="last">
<div>of the model.</div></blockquote>
</dd>
</dl>
<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.accuracy">
<code class="descname">accuracy</code><span class="sig-paren">(</span><em>questions</em>, <em>restrict_vocab=30000</em>, <em>most_similar=&lt;function most_similar&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute accuracy of the model. <cite>questions</cite> is a filename where lines are
4-tuples of words, split into sections by &#8221;: SECTION NAME&#8221; lines.
See <a class="reference external" href="https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt">https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a> for an example.</p>
<p>The accuracy is reported (=printed to log and returned as a list) for each
section separately, plus there&#8217;s one aggregate summary at the end.</p>
<p>Use <cite>restrict_vocab</cite> to ignore all questions containing a word whose frequency
is not in the top-N most frequent words (default top 30,000).</p>
<p>This method corresponds to the <cite>compute-accuracy</cite> script of the original C word2vec.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.build_vocab">
<code class="descname">build_vocab</code><span class="sig-paren">(</span><em>sentences</em>, <em>keep_raw_vocab=False</em>, <em>trim_rule=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.build_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Build vocabulary from a sequence of sentences (can be a once-only generator stream).
Each sentence must be a list of unicode strings.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.clear_sims">
<code class="descname">clear_sims</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.clear_sims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.create_binary_tree">
<code class="descname">create_binary_tree</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.create_binary_tree" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a binary Huffman tree using stored vocabulary word counts. Frequent words
will have shorter binary codes. Called internally from <cite>build_vocab()</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.doesnt_match">
<code class="descname">doesnt_match</code><span class="sig-paren">(</span><em>words</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.doesnt_match" title="Permalink to this definition">¶</a></dt>
<dd><p>Which word from the given list doesn&#8217;t go with the others?</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s">&quot;breakfast cereal dinner lunch&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">&#39;cereal&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.estimate_memory">
<code class="descname">estimate_memory</code><span class="sig-paren">(</span><em>vocab_size=None</em>, <em>report=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.estimate_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate required memory for a model using current settings and provided vocabulary size.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.finalize_vocab">
<code class="descname">finalize_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.finalize_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Build tables and model weights based on final vocabulary settings.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.init_sims">
<code class="descname">init_sims</code><span class="sig-paren">(</span><em>replace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.init_sims" title="Permalink to this definition">¶</a></dt>
<dd><p>Precompute L2-normalized vectors.</p>
<p>If <cite>replace</cite> is set, forget the original vectors and only keep the normalized
ones = saves lots of memory!</p>
<p>Note that you <strong>cannot continue training</strong> after doing a replace. The model becomes
effectively read-only = you can call <cite>most_similar</cite>, <cite>similarity</cite> etc., but not <cite>train</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.intersect_word2vec_format">
<code class="descname">intersect_word2vec_format</code><span class="sig-paren">(</span><em>fname</em>, <em>binary=False</em>, <em>encoding='utf8'</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.intersect_word2vec_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge the input-hidden weight matrix from the original C word2vec-tool format
given, where it intersects with the current vocabulary. (No words are added to the
existing vocabulary, but intersecting words adopt the file&#8217;s weights, and
non-intersecting words are left alone.)</p>
<p><cite>binary</cite> is a boolean indicating whether the data is in binary word2vec format.</p>
</dd></dl>

<dl class="classmethod">
<dt id="gensim.models.word2vec.Word2Vec.load">
<em class="property">classmethod </em><code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.load" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="gensim.models.word2vec.Word2Vec.load_word2vec_format">
<em class="property">classmethod </em><code class="descname">load_word2vec_format</code><span class="sig-paren">(</span><em>fname</em>, <em>fvocab=None</em>, <em>binary=False</em>, <em>norm_only=True</em>, <em>encoding='utf8'</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.load_word2vec_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the input-hidden weight matrix from the original C word2vec-tool format.</p>
<p>Note that the information stored in the file is incomplete (the binary tree is missing),
so while you can query for word similarity etc., you cannot continue training
with a model loaded this way.</p>
<p><cite>binary</cite> is a boolean indicating whether the data is in binary word2vec format.
<cite>norm_only</cite> is a boolean indicating whether to only store normalised word2vec vectors in memory.
Word counts are read from <cite>fvocab</cite> filename, if set (this is the file generated
by <cite>-save-vocab</cite> flag of the original C tool).</p>
<p>If you trained the C model using non-utf8 encoding for words, specify that
encoding in <cite>encoding</cite>.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="gensim.models.word2vec.Word2Vec.log_accuracy">
<em class="property">static </em><code class="descname">log_accuracy</code><span class="sig-paren">(</span><em>section</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.log_accuracy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.make_cum_table">
<code class="descname">make_cum_table</code><span class="sig-paren">(</span><em>power=0.75</em>, <em>domain=2147483647</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.make_cum_table" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a cumulative-distribution table using stored vocabulary word counts for
drawing random words in the negative-sampling training routines.</p>
<p>To draw a word index, choose a random integer up to the maximum value in the
table (cum_table[-1]), then finding that integer&#8217;s sorted insertion point
(as if by bisect_left or ndarray.searchsorted()). That insertion point is the
drawn index, coming up in proportion equal to the increment at that slot.</p>
<p>Called internally from &#8216;build_vocab()&#8217;.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.most_similar">
<code class="descname">most_similar</code><span class="sig-paren">(</span><em>positive=[]</em>, <em>negative=[]</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.most_similar" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words. Positive words contribute positively towards the
similarity, negative words negatively.</p>
<p>This method computes cosine similarity between a simple mean of the projection
weight vectors of the given words and the vectors for each word in the model.
The method corresponds to the <cite>word-analogy</cite> and <cite>distance</cite> scripts in the original
word2vec implementation.</p>
<p>If topn is False, most_similar returns the vector of similarity scores.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;woman&#39;</span><span class="p">,</span> <span class="s">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;man&#39;</span><span class="p">])</span>
<span class="go">[(&#39;queen&#39;, 0.50882536), ...]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.most_similar_cosmul">
<code class="descname">most_similar_cosmul</code><span class="sig-paren">(</span><em>positive=[]</em>, <em>negative=[]</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.most_similar_cosmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words, using the multiplicative combination objective
proposed by Omer Levy and Yoav Goldberg in <a class="footnote-reference" href="#id8" id="id7">[4]</a>. Positive words still contribute
positively towards the similarity, negative words negatively, but with less
susceptibility to one large distance dominating the calculation.</p>
<p>In the common analogy-solving case, of two positive and one negative examples,
this method is equivalent to the &#8220;3CosMul&#8221; objective (equation (4)) of Levy and Goldberg.</p>
<p>Additional positive or negative examples contribute to the numerator or denominator,
respectively – a potentially sensible but untested extension of the method. (With
a single positive example, rankings will be the same as in the default most_similar.)</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;baghdad&#39;</span><span class="p">,</span> <span class="s">&#39;england&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;london&#39;</span><span class="p">])</span>
<span class="go">[(u&#39;iraq&#39;, 0.8488819003105164), ...]</span>
</pre></div>
</div>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[4]</a></td><td>Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.n_similarity">
<code class="descname">n_similarity</code><span class="sig-paren">(</span><em>ws1</em>, <em>ws2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.n_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between two sets of words.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s">&#39;sushi&#39;</span><span class="p">,</span> <span class="s">&#39;shop&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;japanese&#39;</span><span class="p">,</span> <span class="s">&#39;restaurant&#39;</span><span class="p">])</span>
<span class="go">0.61540466561049689</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s">&#39;restaurant&#39;</span><span class="p">,</span> <span class="s">&#39;japanese&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;japanese&#39;</span><span class="p">,</span> <span class="s">&#39;restaurant&#39;</span><span class="p">])</span>
<span class="go">1.0000000000000004</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s">&#39;sushi&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;restaurant&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">&#39;sushi&#39;</span><span class="p">,</span> <span class="s">&#39;restaurant&#39;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.reset_from">
<code class="descname">reset_from</code><span class="sig-paren">(</span><em>other_model</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.reset_from" title="Permalink to this definition">¶</a></dt>
<dd><p>Borrow shareable pre-built structures (like vocab) from the other_model. Useful
if testing multiple models in parallel on the same corpus.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.reset_weights">
<code class="descname">reset_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.reset_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the object to file (also see <cite>load</cite>).</p>
<p><cite>fname_or_handle</cite> is either a string specifying the file name to
save to, or an open file-like object which can be written to. If
the object is a file handle, no special array handling will be
performed; all attributes will be saved to the same file.</p>
<p>If <cite>separately</cite> is None, automatically detect large
numpy/scipy.sparse arrays in the object being stored, and store
them into separate files. This avoids pickle memory errors and
allows mmap&#8217;ing large arrays back on load efficiently.</p>
<p>You can also set <cite>separately</cite> manually, in which case it must be
a list of attribute names to be stored in separate files. The
automatic check is not performed in this case.</p>
<p><cite>ignore</cite> is a set of attribute names to <em>not</em> serialize (file
handles, caches etc). On subsequent load() these attributes will
be set to None.</p>
<p><cite>pickle_protocol</cite> defaults to 2 so the pickled object can be imported
in both Python 2 and 3.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.save_word2vec_format">
<code class="descname">save_word2vec_format</code><span class="sig-paren">(</span><em>fname</em>, <em>fvocab=None</em>, <em>binary=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.save_word2vec_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the input-hidden weight matrix in the same format used by the original
C word2vec-tool, for compatibility.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.scale_vocab">
<code class="descname">scale_vocab</code><span class="sig-paren">(</span><em>min_count=None</em>, <em>sample=None</em>, <em>dry_run=False</em>, <em>keep_raw_vocab=False</em>, <em>trim_rule=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.scale_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply vocabulary settings for <cite>min_count</cite> (discarding less-frequent words)
and <cite>sample</cite> (controlling the downsampling of more-frequent words).</p>
<p>Calling with <cite>dry_run=True</cite> will only simulate the provided settings and
report the size of the retained vocabulary, effective corpus length, and
estimated memory requirements. Results are both printed via logging and
returned as a dict.</p>
<p>Delete the raw vocabulary after the scaling is done to free up RAM,
unless <cite>keep_raw_vocab</cite> is set.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.scan_vocab">
<code class="descname">scan_vocab</code><span class="sig-paren">(</span><em>sentences</em>, <em>progress_per=10000</em>, <em>trim_rule=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.scan_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Do an initial scan of all words appearing in sentences.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>sentences</em>, <em>total_sentences=1000000000</em>, <em>chunksize=100</em>, <em>queue_factor=2</em>, <em>report_delay=1</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the log probability for a sequence of sentences (can be a once-only generator stream).
Each sentence must be a list of unicode strings.
This does not change the fitted model in any way (see Word2Vec.train() for that)</p>
<p>Note that you should specify total_sentences; we&#8217;ll run into problems if you ask to score more than the default</p>
<p>See the article by Taddy <a class="reference internal" href="#taddy" id="id9">[taddy]</a> for examples of how to use such scores in document classification.</p>
<table class="docutils citation" frame="void" id="taddy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[taddy]</a></td><td>Taddy, Matt.  Document Classification by Inversion of Distributed Language Representations, in Proceedings of the 2015 Conference of the Association of Computational Linguistics.</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.seeded_vector">
<code class="descname">seeded_vector</code><span class="sig-paren">(</span><em>seed_string</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.seeded_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Create one &#8216;random&#8217; vector (but deterministic by seed_string)</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.similarity">
<code class="descname">similarity</code><span class="sig-paren">(</span><em>w1</em>, <em>w2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between two words.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">&#39;woman&#39;</span><span class="p">,</span> <span class="s">&#39;man&#39;</span><span class="p">)</span>
<span class="go">0.73723527</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">&#39;woman&#39;</span><span class="p">,</span> <span class="s">&#39;woman&#39;</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.word2vec.Word2Vec.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>sentences</em>, <em>total_words=None</em>, <em>word_count=0</em>, <em>chunksize=100</em>, <em>total_examples=None</em>, <em>queue_factor=2</em>, <em>report_delay=1</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.Word2Vec.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the model&#8217;s neural weights from a sequence of sentences (can be a once-only generator stream).
For Word2Vec, each sentence must be a list of unicode strings. (Subclasses may accept other examples.)</p>
<p>To support linear learning-rate decay from (initial) alpha to min_alpha, either total_examples
(count of sentences) or total_words (count of raw words in sentences) should be provided, unless the
sentences are the same as those that were used to initially build the vocabulary.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="gensim.models.word2vec.score_cbow_pair">
<code class="descclassname">gensim.models.word2vec.</code><code class="descname">score_cbow_pair</code><span class="sig-paren">(</span><em>model</em>, <em>word</em>, <em>word2_indices</em>, <em>l1</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.score_cbow_pair" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="gensim.models.word2vec.score_sg_pair">
<code class="descclassname">gensim.models.word2vec.</code><code class="descname">score_sg_pair</code><span class="sig-paren">(</span><em>model</em>, <em>word</em>, <em>word2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.score_sg_pair" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="gensim.models.word2vec.train_cbow_pair">
<code class="descclassname">gensim.models.word2vec.</code><code class="descname">train_cbow_pair</code><span class="sig-paren">(</span><em>model</em>, <em>word</em>, <em>input_word_indices</em>, <em>l1</em>, <em>alpha</em>, <em>learn_vectors=True</em>, <em>learn_hidden=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.train_cbow_pair" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="gensim.models.word2vec.train_sg_pair">
<code class="descclassname">gensim.models.word2vec.</code><code class="descname">train_sg_pair</code><span class="sig-paren">(</span><em>model</em>, <em>word</em>, <em>context_index</em>, <em>alpha</em>, <em>learn_vectors=True</em>, <em>learn_hidden=True</em>, <em>context_vectors=None</em>, <em>context_locks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.word2vec.train_sg_pair" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>


          </div>
        </div>
      

      <div class="clearer"></div>
    </div>
    

    
    <div id="footer">
      <div id="footerwrapper">
        <div id="footerleft">
          <img src="../_static/images/logo-gensim.png" class="smallerlogo" alt="smaller gensim logo" />
          <a href="../index.html"><img src="../_static/images/gensim-footer.png" alt="gensim footer image" title="Gensim home" /></a>

          <div class="copyright">
            &copy; Copyright 2009-now, <a href="mailto:radimrehurek@seznam.cz" style="color:white"> Radim Řehůřek</a>
            <br />
              Last updated on Sep 19, 2015.
          </div>
        </div>

        <div id="footermiddleright">
          <div id="footermiddle">
            <ul class="navigation">
              <li><a href="../index.html">
                Home
              </a></li>
              <li>|</li>
              <li><a href="../tutorial.html">
                Tutorials
              </a></li>
              <li>|</li>
              <li><a href="../install.html">
                Install
              </a></li>
              <li>|</li>
              <li><a href="../support.html">
                Support
              </a></li>
              <li>|</li>
              <li><a href="../apiref.html">
                API
              </a></li>
              <li>|</li>
              <li><a href="../about.html">
                About
              </a></li>
            </ul>

            <div class="tweetodsazeni">
              <div class="tweet">
                <a href="https://twitter.com/radimrehurek" target="_blank" style="color: white">Tweet @RadimRehurek</a>
              </div>
            </div>

          </div>

          <div id="footerright">
            <div class="footernadpis">
              Support:
            </div>
            <div class="googlegroupsodsazeni">
              <a href="https://groups.google.com/group/gensim" class="googlegroups">
                Stay informed via gensim mailing list:
              </a>

              <form action="http://groups.google.com/group/gensim/boxsubscribe">
                <input type="text" name="email" placeholder="your@email.com" size="28" />
                <input type="submit" name="sub" value="Subscribe" />
              </form>

            </div>

            <div class="addthis_toolbox addthis_default_style addthis_32x32_style"
                addthis:title="#gensim"
                addthis:description="Efficient Topic Modelling in Python"
                style="margin:20px 0 0 0">
              <a class="addthis_button_preferred_1"></a>
              <a class="addthis_button_preferred_2"></a>
              <a class="addthis_button_preferred_3"></a>
              <a class="addthis_button_preferred_4"></a>
              <a class="addthis_button_compact"></a>
              <a class="addthis_counter addthis_bubble_style"></a>
            </div>
          </div>

        </div>
      </div>
    </div>
    

    <script type="text/javascript">
      (function() {
      var at = document.createElement('script'); at.type = 'text/javascript'; at.async = true;
      at.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 's7.addthis.com/js/250/addthis_widget.js#pubid=ra-4d738b9b1d31ccbd';
      var sat = document.getElementsByTagName('script')[0]; sat.parentNode.insertBefore(at, sat);
      })();
    </script>

  </body>
</html>