<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
            "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>The Logistic Regression Model
</TITLE>

<META http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<META name="GENERATOR" content="hevea 1.10">
<STYLE type="text/css">
.li-itemize{margin:1ex 0ex;}
.li-enumerate{margin:1ex 0ex;}
.dd-description{margin:0ex 0ex 1ex 4ex;}
.dt-description{margin:0ex;}
.toc{list-style:none;}
.thefootnotes{text-align:left;margin:0ex;}
.dt-thefootnotes{margin:0em;}
.dd-thefootnotes{margin:0em 0em 0em 2em;}
.footnoterule{margin:1em auto 1em 0px;width:50%;}
.caption{padding-left:2ex; padding-right:2ex; margin-left:auto; margin-right:auto}
.title{margin:2ex auto;text-align:center}
.center{text-align:center;margin-left:auto;margin-right:auto;}
.flushleft{text-align:left;margin-left:0ex;margin-right:auto;}
.flushright{text-align:right;margin-left:auto;margin-right:0ex;}
DIV TABLE{margin-left:inherit;margin-right:inherit;}
PRE{text-align:left;margin-left:0ex;margin-right:auto;}
BLOCKQUOTE{margin-left:4ex;margin-right:4ex;text-align:left;}
TD P{margin:0px;}
.boxed{border:1px solid black}
.textboxed{border:1px solid black}
.vbar{border:none;width:2px;background-color:black;}
.hbar{border:none;height:2px;width:100%;background-color:black;}
.hfill{border:none;height:1px;width:200%;background-color:black;}
.vdisplay{border-collapse:separate;border-spacing:2px;width:auto; empty-cells:show; border:2px solid red;}
.vdcell{white-space:nowrap;padding:0px;width:auto; border:2px solid green;}
.display{border-collapse:separate;border-spacing:2px;width:auto; border:none;}
.dcell{white-space:nowrap;padding:0px;width:auto; border:none;}
.dcenter{margin:0ex auto;}
.vdcenter{border:solid #FF8000 2px; margin:0ex auto;}
.minipage{text-align:left; margin-left:0em; margin-right:auto;}
.marginpar{border:solid thin black; width:20%; text-align:left;}
.marginparleft{float:left; margin-left:0ex; margin-right:1ex;}
.marginparright{float:right; margin-left:1ex; margin-right:0ex;}
.theorem{text-align:left;margin:1ex auto 1ex 0ex;}
.part{margin:2ex auto;text-align:center}
</STYLE>
</HEAD>
<BODY >
<!--HEVEA command line is: c:\cygwin\usr\local\hevea-1.10\hevea -exec xxdate.exe -fix LogisticRegression.tex -->
<!--CUT DEF section 1 --><TABLE CLASS="title"><TR><TD><H1 CLASS="titlemain">The Logistic Regression Model</H1><H3 CLASS="titlerest">Michiel de Hoon (mdehoon@cal.berkeley.edu)</H3></TD></TR>
</TABLE><!--TOC section Contents-->
<H2 CLASS="section"><!--SEC ANCHOR -->Contents</H2><!--SEC END --><UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc1">1&#XA0;&#XA0;Background and Purpose</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc2">2&#XA0;&#XA0;Training the logistic regression model</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc3">3&#XA0;&#XA0;Using the logistic regression model for classification</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc4">4&#XA0;&#XA0;Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc5">5&#XA0;&#XA0;Literature</A>
</LI></UL><!--TOC section Background and Purpose-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc1">1</A>&#XA0;&#XA0;Background and Purpose</H2><!--SEC END --><P>Logistic regression is a supervised learning approach that attempts to distinguish <I>K</I> classes from each other using a weighted sum of some predictor variables <I>x</I><SUB><I>i</I></SUB>. The logistic regression model is used to calculate the weights &#X3B2;<SUB><I>i</I></SUB> of the predictor variables. In Biopython, the logistic regression model is currently implemented for two classes only (<I>K</I> = 2); the number of predictor variables has no predefined limit.</P><P>As an example, let&#X2019;s try to predict the operon structure in bacteria. An operon is a set of adjacent genes on the same strand of DNA that are transcribed into a single mRNA molecule. Translation of the single mRNA molecule then yields the individual proteins. For <I>Bacillus subtilis</I>, whose data we will be using, the average number of genes in an operon is about 2.4.</P><P>As a first step in understanding gene regulation in bacteria, we need to know the operon structure. For about 10% of the genes in <I>Bacillus subtilis</I>, the operon structure is known from experiments. A supervised learning method can be used to predict the operon structure for the remaining 90% of the genes.</P><P>For such a supervised learning approach, we need to choose some predictor variables <I>x</I><SUB><I>i</I></SUB> that can be measured easily and are somehow related to the operon structure. One predictor variable might be the distance in base pairs between genes. Adjacent genes belonging to the same operon tend to be separated by a relatively short distance, whereas adjacent genes in different operons tend to have a larger space between them to allow for promoter and terminator sequences. Another predictor variable is based on gene expression measurements. By definition, genes belonging to the same operon have equal gene expression profiles, while genes in different operons are expected to have different expression profiles. In practice, the measured expression profiles of genes in the same operon are not quite identical due to the presence of measurement errors. To assess the similarity in the gene expression profiles, we assume that the measurement errors follow a normal distribution and calculate the corresponding log-likelihood score.</P><P>We now have two predictor variables that we can use to predict if two adjacent genes on the same strand of DNA belong to the same operon:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<I>x</I><SUB>1</SUB>: the number of base pairs between them;
</LI><LI CLASS="li-itemize"><I>x</I><SUB>2</SUB>: their similarity in expression profile.
</LI></UL><P>In a logistic regression model, we use a weighted sum of these two predictors to calculate a joint score <I>S</I>:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
<I>S</I>&#XA0;=&#XA0;&#X3B2;<SUB>0</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>1</SUB>&#XA0;<I>x</I><SUB>1</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>2</SUB>&#XA0;<I>x</I><SUB>2</SUB>.
&#XA0;&#XA0;&#XA0;&#XA0;(1)</TD></TR>
</TABLE><P>
The logistic regression model gives us appropriate values for the parameters &#X3B2;<SUB>0</SUB>, &#X3B2;<SUB>1</SUB>, &#X3B2;<SUB>2</SUB> using two sets of example genes:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
OP: Adjacent genes, on the same strand of DNA, known to belong to the same operon;
</LI><LI CLASS="li-itemize">NOP: Adjacent genes, on the same strand of DNA, known to belong to different operons.
</LI></UL><P>In the logistic regression model, the probability of belonging to a class depends on the score via the logistic function. For the two classes OP and NOP, we can write this as
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">


&#XA0;&#XA0;&#XA0;&#XA0;&#XA0;

</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=right NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">Pr</TD><TD CLASS="dcell">&#X239B;<BR>
&#X239D;</TD><TD CLASS="dcell"><I>OP</I>|<I>x</I><SUB>1</SUB>,&#XA0;<I>x</I><SUB>2</SUB></TD><TD CLASS="dcell">&#X239E;<BR>
&#X23A0;</TD></TR>
</TABLE></TD><TD ALIGN=center NOWRAP>&#XA0;=</TD><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">&#XA0;</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">exp</TD><TD CLASS="dcell">&#X239B;<BR>
&#X239D;</TD><TD CLASS="dcell">&#X3B2;<SUB>0</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>1</SUB>&#XA0;<I>x</I><SUB>1</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>2</SUB>&#XA0;<I>x</I><SUB>2</SUB></TD><TD CLASS="dcell">&#X239E;<BR>
&#X23A0;</TD></TR>
</TABLE></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">1+exp</TD><TD CLASS="dcell">&#X239B;<BR>
&#X239D;</TD><TD CLASS="dcell">&#X3B2;<SUB>0</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>1</SUB>&#XA0;<I>x</I><SUB>1</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>2</SUB>&#XA0;<I>x</I><SUB>2</SUB></TD><TD CLASS="dcell">&#X239E;<BR>
&#X23A0;</TD></TR>
</TABLE></TD></TR>
</TABLE></TD><TD CLASS="dcell">&#XA0;<A NAME="eq:OP">&#XA0;</A>&#XA0;</TD></TR>
</TABLE></TD><TD ALIGN=right NOWRAP>&#XA0;&#XA0;&#XA0;&#XA0;(2)</TD></TR>
<TR><TD ALIGN=right NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">Pr</TD><TD CLASS="dcell">&#X239B;<BR>
&#X239D;</TD><TD CLASS="dcell"><I>NOP</I>|<I>x</I><SUB>1</SUB>,&#XA0;<I>x</I><SUB>2</SUB></TD><TD CLASS="dcell">&#X239E;<BR>
&#X23A0;</TD></TR>
</TABLE></TD><TD ALIGN=center NOWRAP>&#XA0;=</TD><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">&#XA0;</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">1+exp</TD><TD CLASS="dcell">&#X239B;<BR>
&#X239D;</TD><TD CLASS="dcell">&#X3B2;<SUB>0</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>1</SUB>&#XA0;<I>x</I><SUB>1</SUB>&#XA0;+&#XA0;&#X3B2;<SUB>2</SUB>&#XA0;<I>x</I><SUB>2</SUB></TD><TD CLASS="dcell">&#X239E;<BR>
&#X23A0;</TD></TR>
</TABLE></TD></TR>
</TABLE></TD><TD CLASS="dcell">&#XA0;<A NAME="eq:NOP">&#XA0;</A>&#XA0;
</TD></TR>
</TABLE></TD><TD ALIGN=right NOWRAP>&#XA0;&#XA0;&#XA0;&#XA0;(3)</TD></TR>
</TABLE></TD></TR>
</TABLE><P>
Using a set of gene pairs for which it is known whether they belong to the same operon (class OP) or to different operons (class NOP), we can calculate the weights &#X3B2;<SUB>0</SUB>, &#X3B2;<SUB>1</SUB>, &#X3B2;<SUB>2</SUB> by maximizing the log-likelihood corresponding to the probability functions (<A HREF="#eq:OP">2</A>) and (<A HREF="#eq:NOP">3</A>).</P><!--TOC section Training the logistic regression model-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc2">2</A>&#XA0;&#XA0;Training the logistic regression model</H2><!--SEC END --><BLOCKQUOTE CLASS="table"><DIV CLASS="center"><HR WIDTH="80%" SIZE=2></DIV>
<DIV CLASS="center">
<DIV CLASS="caption"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD VALIGN=top ALIGN=left>Table 1: Adjacent gene pairs known to belong to the same operon (class OP) or to different operons (class NOP). Intergene distances are negative if the two genes overlap.</TD></TR>
</TABLE></DIV>
<TABLE BORDER=1 CELLSPACING=0 CELLPADDING=1><TR><TD ALIGN=center NOWRAP>Gene pair</TD><TD ALIGN=center NOWRAP>Intergene distance (<I>x</I><SUB>1</SUB>)</TD><TD ALIGN=center NOWRAP>Gene expression score (<I>x</I><SUB>2</SUB>)</TD><TD ALIGN=center NOWRAP>Class</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>cotJA</I> &#X2014; <I>cotJB</I></TD><TD ALIGN=center NOWRAP>-53</TD><TD ALIGN=center NOWRAP>-200.78</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yesK</I> &#X2014; <I>yesL</I></TD><TD ALIGN=center NOWRAP>117</TD><TD ALIGN=center NOWRAP>-267.14</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>lplA</I> &#X2014; <I>lplB</I></TD><TD ALIGN=center NOWRAP>57</TD><TD ALIGN=center NOWRAP>-163.47</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>lplB</I> &#X2014; <I>lplC</I></TD><TD ALIGN=center NOWRAP>16</TD><TD ALIGN=center NOWRAP>-190.30</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>lplC</I> &#X2014; <I>lplD</I></TD><TD ALIGN=center NOWRAP>11</TD><TD ALIGN=center NOWRAP>-220.94</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>lplD</I> &#X2014; <I>yetF</I></TD><TD ALIGN=center NOWRAP>85</TD><TD ALIGN=center NOWRAP>-193.94</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yfmT</I> &#X2014; <I>yfmS</I></TD><TD ALIGN=center NOWRAP>16</TD><TD ALIGN=center NOWRAP>-182.71</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yfmF</I> &#X2014; <I>yfmE</I></TD><TD ALIGN=center NOWRAP>15</TD><TD ALIGN=center NOWRAP>-180.41</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>citS</I> &#X2014; <I>citT</I></TD><TD ALIGN=center NOWRAP>-26</TD><TD ALIGN=center NOWRAP>-181.73</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>citM</I> &#X2014; <I>yflN</I></TD><TD ALIGN=center NOWRAP>58</TD><TD ALIGN=center NOWRAP>-259.87</TD><TD ALIGN=center NOWRAP>OP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yfiI</I> &#X2014; <I>yfiJ</I></TD><TD ALIGN=center NOWRAP>126</TD><TD ALIGN=center NOWRAP>-414.53</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>lipB</I> &#X2014; <I>yfiQ</I></TD><TD ALIGN=center NOWRAP>191</TD><TD ALIGN=center NOWRAP>-249.57</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yfiU</I> &#X2014; <I>yfiV</I></TD><TD ALIGN=center NOWRAP>113</TD><TD ALIGN=center NOWRAP>-265.28</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yfhH</I> &#X2014; <I>yfhI</I></TD><TD ALIGN=center NOWRAP>145</TD><TD ALIGN=center NOWRAP>-312.99</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>cotY</I> &#X2014; <I>cotX</I></TD><TD ALIGN=center NOWRAP>154</TD><TD ALIGN=center NOWRAP>-213.83</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yjoB</I> &#X2014; <I>rapA</I></TD><TD ALIGN=center NOWRAP>147</TD><TD ALIGN=center NOWRAP>-380.85</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>ptsI</I> &#X2014; <I>splA</I></TD><TD ALIGN=center NOWRAP>93</TD><TD ALIGN=center NOWRAP>-291.13</TD><TD ALIGN=center NOWRAP>NOP</TD></TR>
</TABLE>
<A NAME="table:training"></A>
</DIV>
<DIV CLASS="center"><HR WIDTH="80%" SIZE=2></DIV></BLOCKQUOTE><P>Table <A HREF="#table:training">1</A> list some of the <I>Bacillus subtilis</I> gene pairs for which the operon structure is known.
Let&#X2019;s calculate the logistic regression model from these data:</P><PRE CLASS="verbatim">&gt;&gt;&gt; from Bio import LogisticRegression
&gt;&gt;&gt; xs = [[-53, -200.78],
          [117, -267.14],
          [57, -163.47],
          [16, -190.30],
          [11, -220.94],
          [85, -193.94],
          [16, -182.71],
          [15, -180.41],
          [-26, -181.73],
          [58, -259.87],
          [126, -414.53],
          [191, -249.57],
          [113, -265.28],
          [145, -312.99],
          [154, -213.83],
          [147, -380.85],
          [93, -291.13]]
&gt;&gt;&gt; ys = [1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0]
&gt;&gt;&gt; model = LogisticRegression.train(xs, ys)
</PRE><P>Here, <CODE>xs</CODE> and <CODE>ys</CODE> are the training data: <CODE>xs</CODE> contains the predictor variables for each gene pair, and <CODE>ys</CODE> specifies if the gene pair belongs to the same operon (<CODE>1</CODE>, class OP) or different operons (<CODE>0</CODE>, class NOP). The resulting logistic regression model is stored in <CODE>model</CODE>, which contains the weights &#X3B2;<SUB>0</SUB>, &#X3B2;<SUB>1</SUB>, and &#X3B2;<SUB>2</SUB>:</P><PRE CLASS="verbatim">&gt;&gt;&gt; model.beta
[8.9830290157144681, -0.035968960444850887, 0.02181395662983519]
</PRE><P>Note that &#X3B2;<SUB>1</SUB> is negative, as gene pairs with a shorter intergene distance have a higher probability of belonging to the same operon (class OP). On the other hand, &#X3B2;<SUB>2</SUB> is positive, as gene pairs belonging to the same operon typically have a higher similarity score of their gene expression profiles.
The parameter &#X3B2;<SUB>0</SUB> is positive due to the higher prevalence of operon gene pairs than non-operon gene pairs in the training data.</P><P>The function <CODE>train</CODE> has two optional arguments: <CODE>update_fn</CODE> and <CODE>typecode</CODE>. The <CODE>update_fn</CODE> can be used to specify a callback function, taking as arguments the iteration number and the log-likelihood. With the callback function, we can for example track the progress of the model calculation (which uses a Newton-Raphson iteration to maximize the log-likelihood function of the logistic regression model):</P><PRE CLASS="verbatim">&gt;&gt;&gt; def show_progress(iteration, loglikelihood):
        print "Iteration:", iteration, "Log-likelihood function:", loglikelihood
&gt;&gt;&gt;
&gt;&gt;&gt; model = LogisticRegression.train(xs, ys, update_fn=show_progress)
Iteration: 0 Log-likelihood function: -11.7835020695
Iteration: 1 Log-likelihood function: -7.15886767672
Iteration: 2 Log-likelihood function: -5.76877209868
Iteration: 3 Log-likelihood function: -5.11362294338
Iteration: 4 Log-likelihood function: -4.74870642433
Iteration: 5 Log-likelihood function: -4.50026077146
Iteration: 6 Log-likelihood function: -4.31127773737
Iteration: 7 Log-likelihood function: -4.16015043396
Iteration: 8 Log-likelihood function: -4.03561719785
Iteration: 9 Log-likelihood function: -3.93073282192
Iteration: 10 Log-likelihood function: -3.84087660929
Iteration: 11 Log-likelihood function: -3.76282560605
Iteration: 12 Log-likelihood function: -3.69425027154
Iteration: 13 Log-likelihood function: -3.6334178602
Iteration: 14 Log-likelihood function: -3.57900855837
Iteration: 15 Log-likelihood function: -3.52999671386
Iteration: 16 Log-likelihood function: -3.48557145163
Iteration: 17 Log-likelihood function: -3.44508206139
Iteration: 18 Log-likelihood function: -3.40799948447
Iteration: 19 Log-likelihood function: -3.3738885624
Iteration: 20 Log-likelihood function: -3.3423876581
Iteration: 21 Log-likelihood function: -3.31319343769
Iteration: 22 Log-likelihood function: -3.2860493346
Iteration: 23 Log-likelihood function: -3.2607366863
Iteration: 24 Log-likelihood function: -3.23706784091
Iteration: 25 Log-likelihood function: -3.21488073614
Iteration: 26 Log-likelihood function: -3.19403459259
Iteration: 27 Log-likelihood function: -3.17440646052
Iteration: 28 Log-likelihood function: -3.15588842703
Iteration: 29 Log-likelihood function: -3.13838533947
Iteration: 30 Log-likelihood function: -3.12181293595
Iteration: 31 Log-likelihood function: -3.10609629966
Iteration: 32 Log-likelihood function: -3.09116857282
Iteration: 33 Log-likelihood function: -3.07696988017
Iteration: 34 Log-likelihood function: -3.06344642288
Iteration: 35 Log-likelihood function: -3.05054971191
Iteration: 36 Log-likelihood function: -3.03823591619
Iteration: 37 Log-likelihood function: -3.02646530573
Iteration: 38 Log-likelihood function: -3.01520177394
Iteration: 39 Log-likelihood function: -3.00441242601
Iteration: 40 Log-likelihood function: -2.99406722296
Iteration: 41 Log-likelihood function: -2.98413867259
</PRE><P>The iteration stops once the increase in the log-likelihood function is less than 0.01. If no convergence is reached after 500 iterations, the <CODE>train</CODE> function returns with an <CODE>AssertionError</CODE>.</P><P>The optional keyword <CODE>typecode</CODE> can almost always be ignored. This keyword allows the user to choose the type of Numeric matrix to use. In particular, to avoid memory problems for very large problems, it may be necessary to use single-precision floats (Float8, Float16, etc.) rather than double, which is used by default.</P><!--TOC section Using the logistic regression model for classification-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc3">3</A>&#XA0;&#XA0;Using the logistic regression model for classification</H2><!--SEC END --><P>Classification is performed by calling the <CODE>classify</CODE> function. Given a logistic regression model and the values for <I>x</I><SUB>1</SUB> and <I>x</I><SUB>2</SUB> (e.g. for a gene pair of unknown operon structure), the <CODE>classify</CODE> function returns <CODE>1</CODE> or <CODE>0</CODE>, corresponding to class OP and class NOP, respectively. For example, let&#X2019;s consider the gene pairs <I>yxcE</I>, <I>yxcD</I> and <I>yxiB</I>, <I>yxiA</I>:</P><BLOCKQUOTE CLASS="table"><DIV CLASS="center"><HR WIDTH="80%" SIZE=2></DIV>
<DIV CLASS="center">
<DIV CLASS="caption"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD VALIGN=top ALIGN=left>Table 2: Adjacent gene pairs of unknown operon status.</TD></TR>
</TABLE></DIV>
<TABLE BORDER=1 CELLSPACING=0 CELLPADDING=1><TR><TD ALIGN=center NOWRAP>Gene pair</TD><TD ALIGN=center NOWRAP>Intergene distance <I>x</I><SUB>1</SUB></TD><TD ALIGN=center NOWRAP>Gene expression score <I>x</I><SUB>2</SUB></TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yxcE</I> &#X2014; <I>yxcD</I></TD><TD ALIGN=center NOWRAP>6</TD><TD ALIGN=center NOWRAP>-173.143442352</TD></TR>
<TR><TD ALIGN=center NOWRAP><I>yxiB</I> &#X2014; <I>yxiA</I></TD><TD ALIGN=center NOWRAP>309</TD><TD ALIGN=center NOWRAP>-271.005880394</TD></TR>
</TABLE>
</DIV>
<DIV CLASS="center"><HR WIDTH="80%" SIZE=2></DIV></BLOCKQUOTE><P>The logistic regression model classifies <I>yxcE</I>, <I>yxcD</I> as belonging to the same operon (class OP), while <I>yxiB</I>, <I>yxiA</I> are predicted to belong to different operons:
</P><PRE CLASS="verbatim">&gt;&gt;&gt; print "yxcE, yxcD:", LogisticRegression.classify(model, [6,-173.143442352])
yxcE, yxcD: 1
&gt;&gt;&gt; print "yxiB, yxiA:", LogisticRegression.classify(model, [309, -271.005880394])
yxiB, yxiA: 0
</PRE><P>(which, by the way, agrees with the biological literature).</P><P>To find out how confident we can be in these predictions, we can call the <CODE>calculate</CODE> function to obtain the probabilities (equations (<A HREF="#eq:OP">2</A>) and <A HREF="#eq:NOP">3</A>) for class OP and NOP. For <I>yxcE</I>, <I>yxcD</I> we find
</P><PRE CLASS="verbatim">&gt;&gt;&gt; q, p = LogisticRegression.calculate(model, [6,-173.143442352])
&gt;&gt;&gt; print "class OP: probability =", p, "class NOP: probability =", q
class OP: probability = 0.993242163503 class NOP: probability = 0.00675783649744
</PRE><P>and for <I>yxiB</I>, <I>yxiA</I>
</P><PRE CLASS="verbatim">&gt;&gt;&gt; q, p = LogisticRegression.calculate(model, [309, -271.005880394])
&gt;&gt;&gt; print "class OP: probability =", p, "class NOP: probability =", q
class OP: probability = 0.000321211251817 class NOP: probability = 0.999678788748
</PRE><P>To get some idea of the prediction accuracy of the logistic regression model, we can apply it to the training data:
</P><PRE CLASS="verbatim">&gt;&gt;&gt; for i in range(len(ys)):
        print "True:", ys[i], "Predicted:", LogisticRegression.classify(model, xs[i])
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
</PRE><P>showing that the prediction is correct for all but one of the gene pairs. A more reliable estimate of the prediction accuracy can be found from a leave-one-out analysis, in which the model is recalculated from the training data after removing the gene to be predicted:
</P><PRE CLASS="verbatim">&gt;&gt;&gt; for i in range(len(ys)):
        model = LogisticRegression.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:])
        print "True:", ys[i], "Predicted:", LogisticRegression.classify(model, xs[i])
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
</PRE><P>The leave-one-out analysis shows the the prediction of the logistic regression model is incorrect for only two of the gene pairs, which corresponds to a prediction accuracy of 88%.</P><!--TOC section Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc4">4</A>&#XA0;&#XA0;Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines</H2><!--SEC END --><P>The logistic regression model is similar to linear discriminant analysis. In linear discriminant analysis, the class probabilities also follow equations (<A HREF="#eq:OP">2</A>) and (<A HREF="#eq:NOP">3</A>). However, instead of estimating the coefficients &#X3B2; directly, we first fit a normal distribution to the predictor variables <I>x</I>. The coefficients &#X3B2; are then calculated from the means and covariances of the normal distribution. If the distribution of <I>x</I> is indeed normal, then we expect linear discriminant analysis to perform better than the logistic regression model. The logistic regression model, on the other hand, is more robust to deviations from normality.</P><P>Another similar approach is a support vector machine with a linear kernel. Such an SVM also uses a linear combination of the predictors, but estimates the coefficients &#X3B2; from the predictor variables <I>x</I> near the boundary region between the classes. If the logistic regression model (equations (<A HREF="#eq:OP">2</A>) and (<A HREF="#eq:NOP">3</A>)) is a good description for <I>x</I> away from the boundary region, we expect the logistic regression model to perform better than an SVM with a linear kernel, as it relies on more data. If not, an SVM with a linear kernel may perform better.</P><!--TOC section Literature-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc5">5</A>&#XA0;&#XA0;Literature</H2><!--SEC END --><P>Trevor Hastie, Robert Tibshirani, and Jerome Friedman: <I>The Elements of Statistical Learning. Data Mining, Inference, and Prediction</I>. Springer Series in Statistics, 2001. Chapter 4.4.</P><!--CUT END -->
<!--HTMLFOOT-->
<!--ENDHTML-->
<!--FOOTER-->
<HR SIZE=2><BLOCKQUOTE CLASS="quote"><EM>This document was translated from L<sup>A</sup>T<sub>E</sub>X by
</EM><A HREF="http://hevea.inria.fr/index.html"><EM>H</EM><EM><FONT SIZE=2><sup>E</sup></FONT></EM><EM>V</EM><EM><FONT SIZE=2><sup>E</sup></FONT></EM><EM>A</EM></A><EM>.</EM></BLOCKQUOTE></BODY>
</HTML>
